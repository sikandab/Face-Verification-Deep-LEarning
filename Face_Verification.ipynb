{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1vV5nTIhLVD"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\"\"\"# **Uploading Dataset from Kaggle and Unzipping the Folders**\"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "kaggle=files.upload()\n",
        "\n",
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!kaggle datasets download -d cmu11785/20fall-hw2p2\n",
        "\n",
        "!unzip -q 20fall-hw2p2.zip\n",
        "\n",
        "\"\"\"# **Defining the Neural Network building blocks**\n",
        "#The building blocks are based upon the base model architecture decribed on Piazza\n",
        "\"\"\"\n",
        "\n",
        "class ConvReLU(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvReLU, self).__init__(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,bias=False),\n",
        "            nn.BatchNorm2d(out_channels, affine = True, track_running_stats = True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p = 0.3)\n",
        "            \n",
        "        )\n",
        "\n",
        "class ConvReLU2(nn.Sequential):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvReLU2, self).__init__(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels, affine = True, track_running_stats = True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "\n",
        "\"\"\"# **Developing the CNN Model**\n",
        "# Built using the two building blocks. I used the base architecture from piazza with an additional Conv2d layer. By adding an extra Conv2d Layer I made my network deeper, hence improving the networks performance\n",
        "\"\"\"\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_feats, hidden_sizes, num_classes, feat_dim=15):\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "        self.hidden_sizes = [num_feats] + hidden_sizes + [num_classes]\n",
        "        \n",
        "        self.layers = []\n",
        "        \n",
        "        self.layers.append(ConvReLU(3, 64, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(64, 64, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(64, 64, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU(64, 128, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(128, 128, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU(128, 256, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(256, 256, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(256, 256, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(ConvReLU2(256, 256, kernel_size = 3, stride = 1))\n",
        "        self.layers.append(nn.Conv2d(256,256,3,1,1))\n",
        "            \n",
        "           \n",
        "    \n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        self.linear_label = nn.Linear(256, num_classes, bias=False)\n",
        "        \n",
        "    \n",
        "    def forward(self, x, evalMode=False):\n",
        "        output = x\n",
        "        output = self.layers(output)\n",
        "            \n",
        "        output = F.max_pool2d(output, [output.size(2), output.size(3)], stride=1)\n",
        "        output = output.reshape(output.shape[0], output.shape[1])\n",
        "        \n",
        "        label_output = self.linear_label(output)\n",
        "        \n",
        "        \n",
        "        \n",
        "        closs_output = output\n",
        "\n",
        "        return closs_output, label_output\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)\n",
        "\n",
        "\"\"\"# **Definied the Training and Classification Method. The code structure is based upon example code given in the 'CNN:Losses, transfer learning' recitation. The recitation occured on OCT. 2nd**\"\"\"\n",
        "\n",
        "def train(model, data_loader, test_loader, task='Classification'):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(numEpochs):\n",
        "        avg_loss = 0.0\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(feats)[1]\n",
        "\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "            \n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "        \n",
        "        if task == 'Classification':\n",
        "            val_loss, val_acc = test_classify(model, test_loader)\n",
        "            train_loss, train_acc = test_classify(model, data_loader)\n",
        "            print('Train Loss: {:.4f}\\tTrain Accuracy: {:.4f}\\tVal Loss: {:.4f}\\tVal Accuracy: {:.4f}'.\n",
        "                  format(train_loss, train_acc, val_loss, val_acc))\n",
        "        else:\n",
        "         \n",
        "            pass\n",
        "\n",
        "\n",
        "def test_classify(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        outputs = model(feats)[1]\n",
        "        \n",
        "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        \n",
        "        loss = criterion(outputs, labels.long())\n",
        "        \n",
        "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "        del feats\n",
        "        del labels\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(test_loss), accuracy/total\n",
        "\n",
        "\"\"\"# **Defining the Co-sine Similarity Method. This Method would be used for the face verication objective. The method outputs a similarity score between a pair of image embeddings optained from the CNN model**\"\"\"\n",
        "\n",
        "def cos_sim(model, final_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        predict=[]\n",
        "\n",
        "        for batch_idx, (data1, data2) in enumerate(final_loader):\n",
        "            data1 = data1.to(device)\n",
        "            data2 = data2.to(device)\n",
        "            img1Embed = model(data1)[0]\n",
        "            img2Embed = model(data2)[0]\n",
        "\n",
        "            simcpu = F.cosine_similarity(img1Embed, img2Embed)\n",
        "            predict = np.concatenate((predict, simcpu.cpu().reshape(-1)))\n",
        "            del data1\n",
        "            del data2\n",
        "\n",
        "    return np.array(predict)\n",
        "\n",
        "\"\"\"# **Loading my Training and Validation Data by utilizing 'Data Loader' and 'Torch Vision'**\"\"\"\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='classification_data/train_data',transform=torchvision.transforms.ToTensor())\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=200, \n",
        "                                               shuffle=True, num_workers=5)\n",
        "\n",
        "dev_dataset = torchvision.datasets.ImageFolder(root='classification_data/val_data', \n",
        "                                               transform=torchvision.transforms.ToTensor())\n",
        "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=200, \n",
        "                                             shuffle=True, num_workers=5)\n",
        "\n",
        "\"\"\"# **Defining: hyperparameters, number of epochs, loss fuction and optimization algorithim**\"\"\"\n",
        "\n",
        "numEpochs = 1\n",
        "num_feats = 10\n",
        "\n",
        "learningRate = 0.15\n",
        "weightDecay = 5e-5\n",
        "\n",
        "hidden_sizes = [64,128, 256, 510]\n",
        "num_classes = len(train_dataset.classes)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "network = Network(num_feats, hidden_sizes, num_classes)\n",
        "network.apply(init_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "\n",
        "\"\"\"# **Initiating the training of the developed network**\"\"\"\n",
        "\n",
        "network.train()\n",
        "network.to(device)\n",
        "train(network, train_dataloader, dev_dataloader)\n",
        "\n",
        "\"\"\"# **Developing a Method that extracts a couple of imgaes from the test data set and inputs them in to the trained network. The network outputs a face embedding for each image. The co-sine similarity method takes the face embeddings as an input and gives a similarity score between both images. This method achieves the verification objective.**\"\"\"\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    file_pairs = Dataset\n",
        "    def __init__(self, file_pairs):\n",
        "        with open(file_pairs) as files:\n",
        "            self.file_list = [line.rstrip() for line in files]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i1, i2 = self.file_list[index].split()\n",
        "        img1 = Image.open(i1)\n",
        "        img2 = Image.open(i2)\n",
        "        img1 = torchvision.transforms.ToTensor()(img1)\n",
        "        img2 = torchvision.transforms.ToTensor()(img2)\n",
        "        return img1, img2 \n",
        "\n",
        "    def getPairs(self):\n",
        "        return self.file_list\n",
        "\n",
        "sub_data = ImageDataset(\"verification_pairs_test.txt\")\n",
        "\n",
        "final_loader = DataLoader(sub_data, shuffle=False, batch_size=200, num_workers=1, pin_memory=True)\n",
        "\n",
        "trial = np.array(sub_data.getPairs())\n",
        "\n",
        "test_score = cos_sim(network, final_loader)\n",
        "\n",
        "\"\"\"# **Saving the results of the verification test on a CSV file**\"\"\"\n",
        "\n",
        "test_score = np.array(test_score)\n",
        "df = pd.DataFrame({\"Id\" : trial, \"Category\" : test_score})\n",
        "df.to_csv(r'./sikandab_HW2P2_4.csv', index=False)\n",
        "\n",
        "\"\"\"# **DISCLAIMER: THE SKELETON OF THE CODE AND VARIOUS FUNCTIONS/CLASSES OF THE CODE ARE BASED UPON EXAMPLE CODE GIVEN IN RECITATION 5 (OCT. 2ND). THE FILE NAMES OF THE EXAMPLE CODE IS: 'recitation'**\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}